{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neuralcoref_json.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XscyUwwWRy8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3551cda1-f3bc-4edc-c4bc-0b3b8073bc0b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLCh2b-0SBkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c1237c-c4f9-4a80-f049-c703cfeeaeee"
      },
      "source": [
        "cd /content/drive/MyDrive/neuralcoref-master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/neuralcoref-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm9aelk0RszG",
        "outputId": "c7833b6c-ed51-4ebf-cb4e-134aebaff3d5"
      },
      "source": [
        "!pwd\n",
        "!pip install -r ./neuralcoref/train/training_requirements.txt -e .\n",
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/neuralcoref-master\n",
            "Obtaining file:///content/drive/MyDrive/neuralcoref-master\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from -r ./neuralcoref/train/training_requirements.txt (line 1)) (2.2.4)\n",
            "Collecting torch<1.4.0,>=1.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/34/2107f342d4493b7107a600ee16005b2870b5a0a5a165bdf5c5e7168a16a6/torch-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (734.6MB)\n",
            "\u001b[K     |████████████████████████████████| 734.6MB 20kB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (1.19.5)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/b2/25e3672636acdf3efc22abc5ff127b9dfb31c7c0ec6de60f336e2dc3fd62/boto3-1.17.80.tar.gz (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (56.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (1.1.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r ./neuralcoref/train/training_requirements.txt (line 3)) (3.12.4)\n",
            "Collecting botocore<1.21.0,>=1.20.80\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/27/dc163256dc8ac1d9ae77622cdddc896f0b51cb81c6d302a3aebce22b134d/botocore-1.20.80-py2.py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 29.3MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (4.0.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX->-r ./neuralcoref/train/training_requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.80->boto3->neuralcoref==4.0) (2.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (3.7.4.3)\n",
            "Building wheels for collected packages: boto3\n",
            "  Building wheel for boto3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for boto3: filename=boto3-1.17.80-py2.py3-none-any.whl size=128919 sha256=0097d9e410d075514e78059afa58990f9cdca4d7ba401a0703b382cbeeb81541\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/9f/b0/342fe7c9ff632bf49e3a90cd8cf481de74f3475691865158e7\n",
            "Successfully built boto3\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.3.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.3.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: botocore 1.20.80 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, tensorboardX, jmespath, botocore, s3transfer, boto3, neuralcoref\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Running setup.py develop for neuralcoref\n",
            "Successfully installed boto3-1.17.80 botocore-1.20.80 jmespath-0.10.0 neuralcoref s3transfer-0.4.2 tensorboardX-2.2 torch-1.3.1\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (56.1.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f97XIO6nRvth",
        "outputId": "82576d78-564d-4b01-8f22-fe57c97afcae"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 35.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (56.1.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.1)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp37-none-any.whl size=829180944 sha256=c0d8785e6f0be8872be58c69493b8004ba0a83b0bea17aeb762a6ff7716037cf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-clf0gii0/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFyE7RlDRz-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5018c3d2-57cc-44b0-e0cb-daa39b7e305e"
      },
      "source": [
        "import spacy\n",
        "import torch\n",
        "import neuralcoref\n",
        "import en_core_web_lg\n",
        "\n",
        "nlp = en_core_web_lg.load()\n",
        "neuralcoref.add_to_pipe(nlp)\n",
        "\n",
        "doc2 = nlp('Angela lives in Boston. She is quite happy in that city.')\n",
        "\n",
        "for ent in doc2.ents:\n",
        "    print(ent._.coref_cluster)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "100%|██████████| 40155833/40155833 [00:02<00:00, 15740407.04B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Angela: [Angela, She]\n",
            "Boston: [Boston, that city]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8T-rL9rVEet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e848fd-541c-4345-e027-86dffe5fc246"
      },
      "source": [
        "!python3 setup.py sdist bdist_wheel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cythonizing sources\n",
            "running sdist\n",
            "running egg_info\n",
            "writing neuralcoref.egg-info/PKG-INFO\n",
            "writing dependency_links to neuralcoref.egg-info/dependency_links.txt\n",
            "writing requirements to neuralcoref.egg-info/requires.txt\n",
            "writing top-level names to neuralcoref.egg-info/top_level.txt\n",
            "adding license file 'LICENSE.txt' (matched pattern 'LICEN[CS]E*')\n",
            "reading manifest file 'neuralcoref.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "writing manifest file 'neuralcoref.egg-info/SOURCES.txt'\n",
            "running check\n",
            "creating neuralcoref-4.0\n",
            "creating neuralcoref-4.0/include\n",
            "creating neuralcoref-4.0/include/msvc9\n",
            "creating neuralcoref-4.0/include/murmurhash\n",
            "creating neuralcoref-4.0/include/numpy\n",
            "creating neuralcoref-4.0/neuralcoref\n",
            "creating neuralcoref-4.0/neuralcoref.egg-info\n",
            "creating neuralcoref-4.0/neuralcoref/tests\n",
            "creating neuralcoref-4.0/neuralcoref/train\n",
            "copying files to neuralcoref-4.0...\n",
            "copying LICENSE.txt -> neuralcoref-4.0\n",
            "copying MANIFEST.in -> neuralcoref-4.0\n",
            "copying README.md -> neuralcoref-4.0\n",
            "copying setup.py -> neuralcoref-4.0\n",
            "copying include/msvc9/stdint.h -> neuralcoref-4.0/include/msvc9\n",
            "copying include/murmurhash/MurmurHash2.h -> neuralcoref-4.0/include/murmurhash\n",
            "copying include/murmurhash/MurmurHash3.h -> neuralcoref-4.0/include/murmurhash\n",
            "copying include/numpy/__multiarray_api.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/__ufunc_api.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/_neighborhood_iterator_imp.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/_numpyconfig.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/arrayobject.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/arrayscalars.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/halffloat.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/ndarrayobject.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/ndarraytypes.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/noprefix.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/npy_3kcompat.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/npy_common.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/npy_cpu.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/npy_deprecated_api.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/npy_endian.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/npy_interrupt.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/npy_math.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/npy_no_deprecated_api.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/npy_os.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/numpyconfig.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/old_defines.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/oldnumeric.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/ufuncobject.h -> neuralcoref-4.0/include/numpy\n",
            "copying include/numpy/utils.h -> neuralcoref-4.0/include/numpy\n",
            "copying neuralcoref/__init__.py -> neuralcoref-4.0/neuralcoref\n",
            "copying neuralcoref/file_utils.py -> neuralcoref-4.0/neuralcoref\n",
            "copying neuralcoref/neuralcoref.cpp -> neuralcoref-4.0/neuralcoref\n",
            "copying neuralcoref.egg-info/PKG-INFO -> neuralcoref-4.0/neuralcoref.egg-info\n",
            "copying neuralcoref.egg-info/SOURCES.txt -> neuralcoref-4.0/neuralcoref.egg-info\n",
            "copying neuralcoref.egg-info/dependency_links.txt -> neuralcoref-4.0/neuralcoref.egg-info\n",
            "copying neuralcoref.egg-info/not-zip-safe -> neuralcoref-4.0/neuralcoref.egg-info\n",
            "copying neuralcoref.egg-info/requires.txt -> neuralcoref-4.0/neuralcoref.egg-info\n",
            "copying neuralcoref.egg-info/top_level.txt -> neuralcoref-4.0/neuralcoref.egg-info\n",
            "copying neuralcoref/tests/__init__.py -> neuralcoref-4.0/neuralcoref/tests\n",
            "copying neuralcoref/tests/test_neuralcoref.py -> neuralcoref-4.0/neuralcoref/tests\n",
            "copying neuralcoref/train/__init__.py -> neuralcoref-4.0/neuralcoref/train\n",
            "copying neuralcoref/train/algorithm.py -> neuralcoref-4.0/neuralcoref/train\n",
            "copying neuralcoref/train/compat.py -> neuralcoref-4.0/neuralcoref/train\n",
            "copying neuralcoref/train/conllparser.py -> neuralcoref-4.0/neuralcoref/train\n",
            "copying neuralcoref/train/dataset.py -> neuralcoref-4.0/neuralcoref/train\n",
            "copying neuralcoref/train/document.py -> neuralcoref-4.0/neuralcoref/train\n",
            "copying neuralcoref/train/evaluator.py -> neuralcoref-4.0/neuralcoref/train\n",
            "copying neuralcoref/train/learn.py -> neuralcoref-4.0/neuralcoref/train\n",
            "copying neuralcoref/train/model.py -> neuralcoref-4.0/neuralcoref/train\n",
            "copying neuralcoref/train/utils.py -> neuralcoref-4.0/neuralcoref/train\n",
            "Writing neuralcoref-4.0/setup.cfg\n",
            "creating dist\n",
            "Creating tar archive\n",
            "removing 'neuralcoref-4.0' (and everything under it)\n",
            "running bdist_wheel\n",
            "running build\n",
            "running build_py\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "creating build/lib.linux-x86_64-3.7/neuralcoref\n",
            "copying neuralcoref/__init__.py -> build/lib.linux-x86_64-3.7/neuralcoref\n",
            "copying neuralcoref/file_utils.py -> build/lib.linux-x86_64-3.7/neuralcoref\n",
            "creating build/lib.linux-x86_64-3.7/neuralcoref/train\n",
            "copying neuralcoref/train/model.py -> build/lib.linux-x86_64-3.7/neuralcoref/train\n",
            "copying neuralcoref/train/document.py -> build/lib.linux-x86_64-3.7/neuralcoref/train\n",
            "copying neuralcoref/train/dataset.py -> build/lib.linux-x86_64-3.7/neuralcoref/train\n",
            "copying neuralcoref/train/algorithm.py -> build/lib.linux-x86_64-3.7/neuralcoref/train\n",
            "copying neuralcoref/train/evaluator.py -> build/lib.linux-x86_64-3.7/neuralcoref/train\n",
            "copying neuralcoref/train/__init__.py -> build/lib.linux-x86_64-3.7/neuralcoref/train\n",
            "copying neuralcoref/train/compat.py -> build/lib.linux-x86_64-3.7/neuralcoref/train\n",
            "copying neuralcoref/train/learn.py -> build/lib.linux-x86_64-3.7/neuralcoref/train\n",
            "copying neuralcoref/train/utils.py -> build/lib.linux-x86_64-3.7/neuralcoref/train\n",
            "copying neuralcoref/train/conllparser.py -> build/lib.linux-x86_64-3.7/neuralcoref/train\n",
            "creating build/lib.linux-x86_64-3.7/neuralcoref/tests\n",
            "copying neuralcoref/tests/__init__.py -> build/lib.linux-x86_64-3.7/neuralcoref/tests\n",
            "copying neuralcoref/tests/test_neuralcoref.py -> build/lib.linux-x86_64-3.7/neuralcoref/tests\n",
            "running build_ext\n",
            "building 'neuralcoref.neuralcoref' extension\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-OGiuun/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-OGiuun/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.7m -I/content/drive/MyDrive/neuralcoref-master/include -I/usr/include/python3.7m -c neuralcoref/neuralcoref.cpp -o build/temp.linux-x86_64-3.7/neuralcoref/neuralcoref.o -O2 -Wno-strict-prototypes -Wno-unused-function\n",
            "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcommand line option ‘\u001b[01m\u001b[K-Wno-strict-prototypes\u001b[m\u001b[K’ is valid for C/ObjC but not for C++\n",
            "In file included from \u001b[01m\u001b[K/content/drive/MyDrive/neuralcoref-master/include/numpy/ndarraytypes.h:1728:0\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/drive/MyDrive/neuralcoref-master/include/numpy/ndarrayobject.h:17\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/drive/MyDrive/neuralcoref-master/include/numpy/arrayobject.h:15\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kneuralcoref/neuralcoref.cpp:612\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/content/drive/MyDrive/neuralcoref-master/include/numpy/npy_deprecated_api.h:11:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it by #defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K-Wcpp\u001b[m\u001b[K]\n",
            " #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it by #defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\"\n",
            "  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kneuralcoref/neuralcoref.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[K__pyx_ctuple_int__and_int __pyx_f_11neuralcoref_11neuralcoref_enlarge_span(__pyx_t_5spacy_7structs_TokenC*, int, int, int, int, __pyx_t_11neuralcoref_11neuralcoref_HashesList)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kneuralcoref/neuralcoref.cpp:8997:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     __pyx_t_5 = ((\u001b[01;35m\u001b[K__pyx_v_maxchild_idx > __pyx_v_sent_start\u001b[m\u001b[K) != 0);\n",
            "                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kneuralcoref/neuralcoref.cpp:9054:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n",
            "     __pyx_t_5 = ((\u001b[01;35m\u001b[K__pyx_v_minchild_idx < (__pyx_v_sent_end - 1)\u001b[m\u001b[K) != 0);\n",
            "                   \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kneuralcoref/neuralcoref.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KPyObject* __pyx_pf_11neuralcoref_11neuralcoref_11NeuralCoref_13predict(__pyx_obj_11neuralcoref_11neuralcoref_NeuralCoref*, PyObject*, float, int, int, PyObject*, int)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kneuralcoref/neuralcoref.cpp:16959:43:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[K__pyx_v_label\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[K(__pyx_v_c[__pyx_v_i]).entity_label = __pyx_f_11neuralcoref_11neuralcoref_get_span_entity_label(((struct __pyx_obj_5spacy_6tokens_4span_Span *)__pyx_t_7))\u001b[m\u001b[K;\n",
            "       \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-OGiuun/python3.7-3.7.10=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/neuralcoref/neuralcoref.o -o build/lib.linux-x86_64-3.7/neuralcoref/neuralcoref.cpython-37m-x86_64-linux-gnu.so\n",
            "installing to build/bdist.linux-x86_64/wheel\n",
            "running install\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/wheel\n",
            "creating build/bdist.linux-x86_64/wheel/neuralcoref\n",
            "copying build/lib.linux-x86_64-3.7/neuralcoref/__init__.py -> build/bdist.linux-x86_64/wheel/neuralcoref\n",
            "copying build/lib.linux-x86_64-3.7/neuralcoref/file_utils.py -> build/bdist.linux-x86_64/wheel/neuralcoref\n",
            "creating build/bdist.linux-x86_64/wheel/neuralcoref/train\n",
            "copying build/lib.linux-x86_64-3.7/neuralcoref/train/model.py -> build/bdist.linux-x86_64/wheel/neuralcoref/train\n",
            "copying build/lib.linux-x86_64-3.7/neuralcoref/train/document.py -> build/bdist.linux-x86_64/wheel/neuralcoref/train\n",
            "copying build/lib.linux-x86_64-3.7/neuralcoref/train/dataset.py -> build/bdist.linux-x86_64/wheel/neuralcoref/train\n",
            "copying build/lib.linux-x86_64-3.7/neuralcoref/train/algorithm.py -> build/bdist.linux-x86_64/wheel/neuralcoref/train\n",
            "copying build/lib.linux-x86_64-3.7/neuralcoref/train/evaluator.py -> build/bdist.linux-x86_64/wheel/neuralcoref/train\n",
            "copying build/lib.linux-x86_64-3.7/neuralcoref/train/__init__.py -> build/bdist.linux-x86_64/wheel/neuralcoref/train\n",
            "copying build/lib.linux-x86_64-3.7/neuralcoref/train/compat.py -> build/bdist.linux-x86_64/wheel/neuralcoref/train\n",
            "copying build/lib.linux-x86_64-3.7/neuralcoref/train/learn.py -> build/bdist.linux-x86_64/wheel/neuralcoref/train\n",
            "copying build/lib.linux-x86_64-3.7/neuralcoref/train/utils.py -> build/bdist.linux-x86_64/wheel/neuralcoref/train\n",
            "copying build/lib.linux-x86_64-3.7/neuralcoref/train/conllparser.py -> build/bdist.linux-x86_64/wheel/neuralcoref/train\n",
            "creating build/bdist.linux-x86_64/wheel/neuralcoref/tests\n",
            "copying build/lib.linux-x86_64-3.7/neuralcoref/tests/__init__.py -> build/bdist.linux-x86_64/wheel/neuralcoref/tests\n",
            "copying build/lib.linux-x86_64-3.7/neuralcoref/tests/test_neuralcoref.py -> build/bdist.linux-x86_64/wheel/neuralcoref/tests\n",
            "copying build/lib.linux-x86_64-3.7/neuralcoref/neuralcoref.cpython-37m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/neuralcoref\n",
            "running install_egg_info\n",
            "Copying neuralcoref.egg-info to build/bdist.linux-x86_64/wheel/neuralcoref-4.0-py3.7.egg-info\n",
            "running install_scripts\n",
            "adding license file \"LICENSE.txt\" (matched pattern \"LICEN[CS]E*\")\n",
            "creating build/bdist.linux-x86_64/wheel/neuralcoref-4.0.dist-info/WHEEL\n",
            "creating 'dist/neuralcoref-4.0-cp37-cp37m-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "adding 'neuralcoref/__init__.py'\n",
            "adding 'neuralcoref/file_utils.py'\n",
            "adding 'neuralcoref/neuralcoref.cpython-37m-x86_64-linux-gnu.so'\n",
            "adding 'neuralcoref/tests/__init__.py'\n",
            "adding 'neuralcoref/tests/test_neuralcoref.py'\n",
            "adding 'neuralcoref/train/__init__.py'\n",
            "adding 'neuralcoref/train/algorithm.py'\n",
            "adding 'neuralcoref/train/compat.py'\n",
            "adding 'neuralcoref/train/conllparser.py'\n",
            "adding 'neuralcoref/train/dataset.py'\n",
            "adding 'neuralcoref/train/document.py'\n",
            "adding 'neuralcoref/train/evaluator.py'\n",
            "adding 'neuralcoref/train/learn.py'\n",
            "adding 'neuralcoref/train/model.py'\n",
            "adding 'neuralcoref/train/utils.py'\n",
            "adding 'neuralcoref-4.0.dist-info/LICENSE.txt'\n",
            "adding 'neuralcoref-4.0.dist-info/METADATA'\n",
            "adding 'neuralcoref-4.0.dist-info/WHEEL'\n",
            "adding 'neuralcoref-4.0.dist-info/top_level.txt'\n",
            "adding 'neuralcoref-4.0.dist-info/RECORD'\n",
            "removing build/bdist.linux-x86_64/wheel\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdRYMFmCVZft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b1bdbe-c0a6-4b61-ce4c-a9c377835e81"
      },
      "source": [
        "pip install pyjsonnlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyjsonnlp in /usr/local/lib/python3.7/dist-packages (0.2.33)\n",
            "Requirement already satisfied: iso639 in /usr/local/lib/python3.7/dist-packages (from pyjsonnlp) (0.1.4)\n",
            "Requirement already satisfied: aioify>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from pyjsonnlp) (0.4.0)\n",
            "Requirement already satisfied: conllu>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from pyjsonnlp) (4.4)\n",
            "Requirement already satisfied: jsonschemanlplab>=3.0.1.1 in /usr/local/lib/python3.7/dist-packages (from pyjsonnlp) (3.0.1.1)\n",
            "Requirement already satisfied: syntok>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from pyjsonnlp) (1.3.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from pyjsonnlp) (1.1.4)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (from pyjsonnlp) (0.0.1)\n",
            "Requirement already satisfied: module-wrapper<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from aioify>=0.3.1->pyjsonnlp) (0.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from jsonschemanlplab>=3.0.1.1->pyjsonnlp) (56.1.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschemanlplab>=3.0.1.1->pyjsonnlp) (21.2.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschemanlplab>=3.0.1.1->pyjsonnlp) (0.17.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from jsonschemanlplab>=3.0.1.1->pyjsonnlp) (1.15.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from syntok>=1.1.1->pyjsonnlp) (2019.12.20)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->pyjsonnlp) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->pyjsonnlp) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->pyjsonnlp) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->pyjsonnlp) (7.1.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->pyjsonnlp) (4.6.3)\n",
            "Requirement already satisfied: stdlib_list<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from module-wrapper<0.4.0,>=0.3.0->aioify>=0.3.1->pyjsonnlp) (0.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->pyjsonnlp) (2.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkbCmt7aUbAu"
      },
      "source": [
        "import pyjsonnlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6LMhMixXhBb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb40fe98-186c-4df2-e8d9-fbb5760268d1"
      },
      "source": [
        "pip install stanza\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stanza in /usr/local/lib/python3.7/dist-packages (1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.3.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2020.12.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (56.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5HRsWTtXb7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a615568-9f25-4fc3-9c15-d6b574b7f652"
      },
      "source": [
        "import stanza\n",
        "stanza.download('en')\n",
        "nlp = stanza.Pipeline('en', processors='tokenize,pos')\n",
        "doc = nlp('Test sentence.') # doc is class Documen"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 43.1MB/s]                    \n",
            "2021-05-26 11:11:15 INFO: Downloading default packages for language: en (English)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/default.zip: 100%|██████████| 411M/411M [01:17<00:00, 5.28MB/s]\n",
            "2021-05-26 11:12:40 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "2021-05-26 11:12:40 INFO: Loading these models for language: en (English):\n",
            "========================\n",
            "| Processor | Package  |\n",
            "------------------------\n",
            "| tokenize  | combined |\n",
            "| pos       | combined |\n",
            "========================\n",
            "\n",
            "2021-05-26 11:12:41 INFO: Use device: cpu\n",
            "2021-05-26 11:12:41 INFO: Loading: tokenize\n",
            "2021-05-26 11:12:41 INFO: Loading: pos\n",
            "2021-05-26 11:12:41 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lud_FcSmYMWs"
      },
      "source": [
        "import json\n",
        "with open('/friends-train.json', 'r') as inputfile:\n",
        "    y = json.loads(inputfile.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJBJYzCtbCVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20276eab-f71a-41e1-b0cf-69e451d7ca0a"
      },
      "source": [
        "y.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['season_id', 'episodes'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQLW9Zobd2yk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757c8c1a-ae46-4d77-e1ac-e4e1ff659448"
      },
      "source": [
        "pip install ijson"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ijson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0c/e3b7bf52e23345d5f9a6a3ff6de0cad419c96491893ab60cbbe9161644a8/ijson-3.1.4-cp37-cp37m-manylinux2010_x86_64.whl (126kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 14.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 21.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 12.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71kB 8.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 81kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 8.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 112kB 7.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122kB 7.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 7.6MB/s \n",
            "\u001b[?25hInstalling collected packages: ijson\n",
            "Successfully installed ijson-3.1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHivs-Fnd12o"
      },
      "source": [
        "import ijson\n",
        "parser = ijson.parse(open(\"/friends-train.json\"))\n",
        "#for prefix, type_of_object, value in ijson.parse(open(\"/friends-train.json\")):\n",
        "#    print(prefix, type_of_object, value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA3AtbaTbxp2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80b95db1-2c9f-4d87-c4a1-1daec229130c"
      },
      "source": [
        "cnt = 0\n",
        "for el in parser:\n",
        "  print(el)\n",
        "  cnt += 1\n",
        "  if cnt == 10:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('episodes.item.scenes.item.utterances.item.character_entities.item.item', 'start_array', None)\n",
            "('episodes.item.scenes.item.utterances.item.character_entities.item.item.item', 'number', 8)\n",
            "('episodes.item.scenes.item.utterances.item.character_entities.item.item.item', 'number', 9)\n",
            "('episodes.item.scenes.item.utterances.item.character_entities.item.item.item', 'string', 'Monica Geller')\n",
            "('episodes.item.scenes.item.utterances.item.character_entities.item.item', 'end_array', None)\n",
            "('episodes.item.scenes.item.utterances.item.character_entities.item', 'end_array', None)\n",
            "('episodes.item.scenes.item.utterances.item.character_entities', 'end_array', None)\n",
            "('episodes.item.scenes.item.utterances.item', 'end_map', None)\n",
            "('episodes.item.scenes.item.utterances.item', 'start_map', None)\n",
            "('episodes.item.scenes.item.utterances.item', 'map_key', 'utterance_id')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVmNsySkWT8I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "7c2d6bd3-bbc6-4f71-eb38-b5cf224dd10e"
      },
      "source": [
        "from stanza.utils.conll import CoNLL\n",
        "conll = CoNLL.convert_dict(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-c0458cc50a42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconll\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoNLL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoNLL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stanza/utils/conll.py\u001b[0m in \u001b[0;36mconvert_dict\u001b[0;34m(doc_dict)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0msent_conll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0mtoken_conll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoNLL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_token_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0msent_conll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_conll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mdoc_conll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_conll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stanza/utils/conll.py\u001b[0m in \u001b[0;36mconvert_token_dict\u001b[0;34m(token_dict)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# when a word (not mwt token) without head is found, we insert dummy head as required by the UD eval script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken_conll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFIELD_TO_IDX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mHEAD\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mtoken_conll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFIELD_TO_IDX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mHEAD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtoken_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# evaluation script requires head: int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtoken_conll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yObKOGCqnGiC",
        "outputId": "19573d8a-c056-45ff-f440-459d3e17f877"
      },
      "source": [
        "!python -m neuralcoref.train.conllparser --path /content/train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "Loading embeddings from /content/drive/MyDrive/neuralcoref-master/neuralcoref/train/weights/static_word\n",
            "Loading embeddings from /content/drive/MyDrive/neuralcoref-master/neuralcoref/train/weights/tuned_word\n",
            "🌋 Reading files\n",
            "In /content/train /content/train\n",
            "['/content/train/friends.train.scene_delim.conll']\n",
            "In /content/train/.ipynb_checkpoints /content/train/.ipynb_checkpoints\n",
            "[]\n",
            "In /content/train/numpy /content/train/numpy\n",
            "[]\n",
            "utts_text size 13556\n",
            "utts_tokens size 13556\n",
            "utts_corefs size 13556\n",
            "utts_speakers size 13556\n",
            "utts_doc_idx size 13556\n",
            "🌋 Building docs\n",
            "🌋 Loading spacy model\n",
            "\u001b[1m\n",
            "===================== Info about model 'en_core_web_lg' =====================\u001b[0m\n",
            "\n",
            "lang             en                            \n",
            "name             core_web_lg                   \n",
            "license          MIT                           \n",
            "author           Explosion                     \n",
            "url              https://explosion.ai          \n",
            "email            contact@explosion.ai          \n",
            "description      English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities.\n",
            "sources          [{'name': 'OntoNotes 5', 'url': 'https://catalog.ldc.upenn.edu/LDC2013T19', 'license': 'commercial (licensed by Explosion)'}, {'name': 'Common Crawl'}]\n",
            "pipeline         ['tagger', 'parser', 'ner']   \n",
            "version          2.2.5                         \n",
            "spacy_version    >=2.2.2                       \n",
            "parent_package   spacy                         \n",
            "labels           {'tagger': ['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``'], 'parser': ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp'], 'ner': ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']}\n",
            "vectors          {'width': 300, 'vectors': 684831, 'keys': 684830, 'name': 'en_core_web_lg.vectors'}\n",
            "source           /usr/local/lib/python3.7/dist-packages/en_core_web_lg\n",
            "\n",
            "Loading model en_core_web_lg\n",
            "🌋 Parsing utterances and filling docs with use_gold_mentions=False\n",
            "13556it [01:16, 177.75it/s]\n",
            "=> read_corpus time elapsed 89.1783094406128\n",
            "🌋 Extracting mentions features with 1 job(s)\n",
            "100% 364/364 [00:14<00:00, 24.50it/s]\n",
            "🌋 Building and gathering array with 1 job(s)\n",
            "100% 364/364 [00:58<00:00,  6.24it/s]\n",
            "100% 374/374 [00:00<00:00, 1021.13it/s]\n",
            "Building numpy array for mentions_features length 34318\n",
            "Saving numpy mentions_features size (34318, 6)\n",
            "Building numpy array for mentions_labels length 34318\n",
            "Saving numpy mentions_labels size (34318, 1)\n",
            "Building numpy array for mentions_pairs_length length 34318\n",
            "Saving numpy mentions_pairs_length size (34318, 1)\n",
            "Building numpy array for mentions_pairs_start_index length 34318\n",
            "Saving numpy mentions_pairs_start_index size (34318, 1)\n",
            "Building numpy array for mentions_spans length 34318\n",
            "Saving numpy mentions_spans size (34318, 250)\n",
            "Building numpy array for mentions_words length 34318\n",
            "Saving numpy mentions_words size (34318, 8)\n",
            "Building numpy array for pairs_ant_index length 1970646\n",
            "Saving numpy pairs_ant_index size (1970646, 1)\n",
            "Building numpy array for pairs_features length 1970646\n",
            "Saving numpy pairs_features size (1970646, 9)\n",
            "Building numpy array for pairs_labels length 1970646\n",
            "Saving numpy pairs_labels size (1970646, 1)\n",
            "Saving pickle locations size 374\n",
            "Saving pickle conll_tokens size 374\n",
            "Saving pickle spacy_lookup size 374\n",
            "Saving pickle doc size 374\n",
            "=> build_and_gather_multiple_arrays time elapsed 81.58329844474792\n",
            "🌋 Building tunable vocabulary matrix from static vocabulary\n",
            "🌋 Saving vocabulary\n",
            "🌋 Saving vocabulary\n",
            "Saving tunable voc, size: (34288, 50)\n",
            "Saving static voc, size: (103144, 50)\n",
            "=> save_vocabulary time elapsed 0.34967756271362305\n",
            "=> total time elapsed 171.11146688461304\n",
            "🌋 Building key file from corpus\n",
            "Saving in /content/train/key.txt\n",
            "In /content/train\n",
            "In /content/train/.ipynb_checkpoints\n",
            "In /content/train/numpy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3A9DAw0n3pw",
        "outputId": "02a4dfa1-4677-4c79-9f28-b285972a4c23"
      },
      "source": [
        "!python -m neuralcoref.train.conllparser --path /content/dev"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "Loading embeddings from /content/drive/MyDrive/neuralcoref-master/neuralcoref/train/weights/static_word\n",
            "Loading embeddings from /content/drive/MyDrive/neuralcoref-master/neuralcoref/train/weights/tuned_word\n",
            "🌋 Reading files\n",
            "In /content/dev /content/dev\n",
            "['/content/dev/friends.test.scene_delim.conll']\n",
            "In /content/dev/.ipynb_checkpoints /content/dev/.ipynb_checkpoints\n",
            "[]\n",
            "In /content/dev/numpy /content/dev/numpy\n",
            "[]\n",
            "utts_text size 2504\n",
            "utts_tokens size 2504\n",
            "utts_corefs size 2504\n",
            "utts_speakers size 2504\n",
            "utts_doc_idx size 2504\n",
            "🌋 Building docs\n",
            "🌋 Loading spacy model\n",
            "\u001b[1m\n",
            "===================== Info about model 'en_core_web_lg' =====================\u001b[0m\n",
            "\n",
            "lang             en                            \n",
            "name             core_web_lg                   \n",
            "license          MIT                           \n",
            "author           Explosion                     \n",
            "url              https://explosion.ai          \n",
            "email            contact@explosion.ai          \n",
            "description      English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities.\n",
            "sources          [{'name': 'OntoNotes 5', 'url': 'https://catalog.ldc.upenn.edu/LDC2013T19', 'license': 'commercial (licensed by Explosion)'}, {'name': 'Common Crawl'}]\n",
            "pipeline         ['tagger', 'parser', 'ner']   \n",
            "version          2.2.5                         \n",
            "spacy_version    >=2.2.2                       \n",
            "parent_package   spacy                         \n",
            "labels           {'tagger': ['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``'], 'parser': ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp'], 'ner': ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']}\n",
            "vectors          {'width': 300, 'vectors': 684831, 'keys': 684830, 'name': 'en_core_web_lg.vectors'}\n",
            "source           /usr/local/lib/python3.7/dist-packages/en_core_web_lg\n",
            "\n",
            "Loading model en_core_web_lg\n",
            "🌋 Parsing utterances and filling docs with use_gold_mentions=False\n",
            "2504it [00:14, 178.69it/s]\n",
            "=> read_corpus time elapsed 26.04082179069519\n",
            "🌋 Extracting mentions features with 1 job(s)\n",
            "100% 64/64 [00:02<00:00, 27.49it/s]\n",
            "🌋 Building and gathering array with 1 job(s)\n",
            "100% 64/64 [00:07<00:00,  8.19it/s]\n",
            "100% 74/74 [00:00<00:00, 1988.52it/s]\n",
            "Building numpy array for mentions_features length 6090\n",
            "Saving numpy mentions_features size (6090, 6)\n",
            "Building numpy array for mentions_labels length 6090\n",
            "Saving numpy mentions_labels size (6090, 1)\n",
            "Building numpy array for mentions_pairs_length length 6090\n",
            "Saving numpy mentions_pairs_length size (6090, 1)\n",
            "Building numpy array for mentions_pairs_start_index length 6090\n",
            "Saving numpy mentions_pairs_start_index size (6090, 1)\n",
            "Building numpy array for mentions_spans length 6090\n",
            "Saving numpy mentions_spans size (6090, 250)\n",
            "Building numpy array for mentions_words length 6090\n",
            "Saving numpy mentions_words size (6090, 8)\n",
            "Building numpy array for pairs_ant_index length 315432\n",
            "Saving numpy pairs_ant_index size (315432, 1)\n",
            "Building numpy array for pairs_features length 315432\n",
            "Saving numpy pairs_features size (315432, 9)\n",
            "Building numpy array for pairs_labels length 315432\n",
            "Saving numpy pairs_labels size (315432, 1)\n",
            "Saving pickle locations size 74\n",
            "Saving pickle conll_tokens size 74\n",
            "Saving pickle spacy_lookup size 74\n",
            "Saving pickle doc size 74\n",
            "=> build_and_gather_multiple_arrays time elapsed 12.302427768707275\n",
            "🌋 Building tunable vocabulary matrix from static vocabulary\n",
            "🌋 Saving vocabulary\n",
            "🌋 Saving vocabulary\n",
            "Saving tunable voc, size: (34288, 50)\n",
            "Saving static voc, size: (103144, 50)\n",
            "=> save_vocabulary time elapsed 0.33161449432373047\n",
            "=> total time elapsed 38.675034284591675\n",
            "🌋 Building key file from corpus\n",
            "Saving in /content/dev/key.txt\n",
            "In /content/dev\n",
            "In /content/dev/.ipynb_checkpoints\n",
            "In /content/dev/numpy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBqFGC2FotQZ",
        "outputId": "06d46929-8997-483e-c77a-e24b3c590e0d"
      },
      "source": [
        "!python -m neuralcoref.train.learn --train /content/train --eval /content/dev"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "Namespace(all_pairs_epoch=200, all_pairs_l2=1e-06, all_pairs_lr=0.0002, batchsize=20000, checkpoint_file=None, conll_eval_interval=10, conll_train_interval=20, costfl=0.4, costfn=0.8, costs={'FN': 0.8, 'FL': 0.4, 'WL': 1.0}, costwl=1.0, cuda=False, eval='/content/dev/numpy/', evalkey='/content/dev/key.txt', h1=1000, h2=500, h3=500, lazy=True, log_interval=10, min_lr=2e-08, numworkers=8, on_eval_decrease='nothing', patience=3, ranking_epoch=200, ranking_l2=1e-05, ranking_lr=2e-06, save_path='/content/drive/MyDrive/neuralcoref-master/neuralcoref/train/checkpoints/May20_20-16-16_8e141abbce67_', seed=1111, startstage=None, startstep=None, top_pairs_epoch=200, top_pairs_l2=1e-05, top_pairs_lr=0.0002, train='/content/train/numpy/', trainkey='/content/train/key.txt', weights=None)\n",
            "Training for 200 200 200 epochs\n",
            "loading /content/train/numpy/tuned_word_embeddings.npy\n",
            "torch.Size([34288, 50])\n",
            "loading /content/train/numpy/tuned_word_vocabulary.txt\n",
            "🏝 Loading Dataset at /content/train/numpy/\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/drive/MyDrive/neuralcoref-master/neuralcoref/train/learn.py\", line 565, in <module>\n",
            "    run_model(args)\n",
            "  File \"/content/drive/MyDrive/neuralcoref-master/neuralcoref/train/learn.py\", line 136, in run_model\n",
            "    dataset = NCDataset(args.train, args)\n",
            "  File \"/content/drive/MyDrive/neuralcoref-master/neuralcoref/train/dataset.py\", line 82, in __init__\n",
            "    data_path + file_name, mmap_mode=\"r\" if params.lazy else None\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\", line 437, in load\n",
            "    return format.open_memmap(file, mode=mmap_mode)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/lib/format.py\", line 858, in open_memmap\n",
            "    raise ValueError(msg)\n",
            "ValueError: Array can't be memory-mapped: Python objects in dtype.\n",
            "Reading mentions_pairs_length.npy, mentions_labels.npy, mentions_pairs_start_index.npy, pairs_features.npy, tuned_word_embeddings.npy, mentions_words.npy, mentions_spans.npy, mentions_features.npy, "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}