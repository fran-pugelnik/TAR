{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neuralcoref.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvpM6uW_Ri11",
        "outputId": "19d90ef3-812c-4c38-8674-bd992caf4422"
      },
      "source": [
        "cd /content/drive/MyDrive/neuralcoref-master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/neuralcoref-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm9aelk0RszG",
        "outputId": "a53d9263-ede0-4d7b-cc45-959f35405fa1"
      },
      "source": [
        "!pwd\n",
        "!pip install -r ./neuralcoref/train/training_requirements.txt -e .\n",
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/neuralcoref-master\n",
            "Obtaining file:///content/drive/MyDrive/neuralcoref-master\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from -r ./neuralcoref/train/training_requirements.txt (line 1)) (2.2.4)\n",
            "Requirement already satisfied: torch<1.4.0,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r ./neuralcoref/train/training_requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (from -r ./neuralcoref/train/training_requirements.txt (line 3)) (2.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (1.19.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (1.17.76)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (0.8.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (56.1.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (7.4.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r ./neuralcoref/train/training_requirements.txt (line 3)) (3.12.4)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.76 in /usr/local/lib/python3.7/dist-packages (from boto3->neuralcoref==4.0) (1.20.76)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->neuralcoref==4.0) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->neuralcoref==4.0) (0.4.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (4.0.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX->-r ./neuralcoref/train/training_requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.76->boto3->neuralcoref==4.0) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (3.4.1)\n",
            "Installing collected packages: neuralcoref\n",
            "  Found existing installation: neuralcoref 4.0\n",
            "    Can't uninstall 'neuralcoref'. No files were found to uninstall.\n",
            "  Running setup.py develop for neuralcoref\n",
            "Successfully installed neuralcoref\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (56.1.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f97XIO6nRvth",
        "outputId": "0d5390ea-5e37-4f49-da93-9509c011cf2d"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (56.1.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFyE7RlDRz-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f993ca07-5ab7-4814-a192-1789a4c8ee37"
      },
      "source": [
        "import spacy\n",
        "import torch\n",
        "import neuralcoref\n",
        "import en_core_web_lg\n",
        "\n",
        "nlp = en_core_web_lg.load()\n",
        "neuralcoref.add_to_pipe(nlp)\n",
        "\n",
        "doc2 = nlp('Angela lives in Boston. She is quite happy in that city.')\n",
        "\n",
        "for ent in doc2.ents:\n",
        "    print(ent._.coref_cluster)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "100%|██████████| 40155833/40155833 [00:01<00:00, 39255163.57B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Angela: [Angela, She]\n",
            "Boston: [Boston, that city]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yObKOGCqnGiC",
        "outputId": "19573d8a-c056-45ff-f440-459d3e17f877"
      },
      "source": [
        "!python -m neuralcoref.train.conllparser --path /content/train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "Loading embeddings from /content/drive/MyDrive/neuralcoref-master/neuralcoref/train/weights/static_word\n",
            "Loading embeddings from /content/drive/MyDrive/neuralcoref-master/neuralcoref/train/weights/tuned_word\n",
            "🌋 Reading files\n",
            "In /content/train /content/train\n",
            "['/content/train/friends.train.scene_delim.conll']\n",
            "In /content/train/.ipynb_checkpoints /content/train/.ipynb_checkpoints\n",
            "[]\n",
            "In /content/train/numpy /content/train/numpy\n",
            "[]\n",
            "utts_text size 13556\n",
            "utts_tokens size 13556\n",
            "utts_corefs size 13556\n",
            "utts_speakers size 13556\n",
            "utts_doc_idx size 13556\n",
            "🌋 Building docs\n",
            "🌋 Loading spacy model\n",
            "\u001b[1m\n",
            "===================== Info about model 'en_core_web_lg' =====================\u001b[0m\n",
            "\n",
            "lang             en                            \n",
            "name             core_web_lg                   \n",
            "license          MIT                           \n",
            "author           Explosion                     \n",
            "url              https://explosion.ai          \n",
            "email            contact@explosion.ai          \n",
            "description      English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities.\n",
            "sources          [{'name': 'OntoNotes 5', 'url': 'https://catalog.ldc.upenn.edu/LDC2013T19', 'license': 'commercial (licensed by Explosion)'}, {'name': 'Common Crawl'}]\n",
            "pipeline         ['tagger', 'parser', 'ner']   \n",
            "version          2.2.5                         \n",
            "spacy_version    >=2.2.2                       \n",
            "parent_package   spacy                         \n",
            "labels           {'tagger': ['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``'], 'parser': ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp'], 'ner': ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']}\n",
            "vectors          {'width': 300, 'vectors': 684831, 'keys': 684830, 'name': 'en_core_web_lg.vectors'}\n",
            "source           /usr/local/lib/python3.7/dist-packages/en_core_web_lg\n",
            "\n",
            "Loading model en_core_web_lg\n",
            "🌋 Parsing utterances and filling docs with use_gold_mentions=False\n",
            "13556it [01:16, 177.75it/s]\n",
            "=> read_corpus time elapsed 89.1783094406128\n",
            "🌋 Extracting mentions features with 1 job(s)\n",
            "100% 364/364 [00:14<00:00, 24.50it/s]\n",
            "🌋 Building and gathering array with 1 job(s)\n",
            "100% 364/364 [00:58<00:00,  6.24it/s]\n",
            "100% 374/374 [00:00<00:00, 1021.13it/s]\n",
            "Building numpy array for mentions_features length 34318\n",
            "Saving numpy mentions_features size (34318, 6)\n",
            "Building numpy array for mentions_labels length 34318\n",
            "Saving numpy mentions_labels size (34318, 1)\n",
            "Building numpy array for mentions_pairs_length length 34318\n",
            "Saving numpy mentions_pairs_length size (34318, 1)\n",
            "Building numpy array for mentions_pairs_start_index length 34318\n",
            "Saving numpy mentions_pairs_start_index size (34318, 1)\n",
            "Building numpy array for mentions_spans length 34318\n",
            "Saving numpy mentions_spans size (34318, 250)\n",
            "Building numpy array for mentions_words length 34318\n",
            "Saving numpy mentions_words size (34318, 8)\n",
            "Building numpy array for pairs_ant_index length 1970646\n",
            "Saving numpy pairs_ant_index size (1970646, 1)\n",
            "Building numpy array for pairs_features length 1970646\n",
            "Saving numpy pairs_features size (1970646, 9)\n",
            "Building numpy array for pairs_labels length 1970646\n",
            "Saving numpy pairs_labels size (1970646, 1)\n",
            "Saving pickle locations size 374\n",
            "Saving pickle conll_tokens size 374\n",
            "Saving pickle spacy_lookup size 374\n",
            "Saving pickle doc size 374\n",
            "=> build_and_gather_multiple_arrays time elapsed 81.58329844474792\n",
            "🌋 Building tunable vocabulary matrix from static vocabulary\n",
            "🌋 Saving vocabulary\n",
            "🌋 Saving vocabulary\n",
            "Saving tunable voc, size: (34288, 50)\n",
            "Saving static voc, size: (103144, 50)\n",
            "=> save_vocabulary time elapsed 0.34967756271362305\n",
            "=> total time elapsed 171.11146688461304\n",
            "🌋 Building key file from corpus\n",
            "Saving in /content/train/key.txt\n",
            "In /content/train\n",
            "In /content/train/.ipynb_checkpoints\n",
            "In /content/train/numpy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3A9DAw0n3pw",
        "outputId": "02a4dfa1-4677-4c79-9f28-b285972a4c23"
      },
      "source": [
        "!python -m neuralcoref.train.conllparser --path /content/dev"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "Loading embeddings from /content/drive/MyDrive/neuralcoref-master/neuralcoref/train/weights/static_word\n",
            "Loading embeddings from /content/drive/MyDrive/neuralcoref-master/neuralcoref/train/weights/tuned_word\n",
            "🌋 Reading files\n",
            "In /content/dev /content/dev\n",
            "['/content/dev/friends.test.scene_delim.conll']\n",
            "In /content/dev/.ipynb_checkpoints /content/dev/.ipynb_checkpoints\n",
            "[]\n",
            "In /content/dev/numpy /content/dev/numpy\n",
            "[]\n",
            "utts_text size 2504\n",
            "utts_tokens size 2504\n",
            "utts_corefs size 2504\n",
            "utts_speakers size 2504\n",
            "utts_doc_idx size 2504\n",
            "🌋 Building docs\n",
            "🌋 Loading spacy model\n",
            "\u001b[1m\n",
            "===================== Info about model 'en_core_web_lg' =====================\u001b[0m\n",
            "\n",
            "lang             en                            \n",
            "name             core_web_lg                   \n",
            "license          MIT                           \n",
            "author           Explosion                     \n",
            "url              https://explosion.ai          \n",
            "email            contact@explosion.ai          \n",
            "description      English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities.\n",
            "sources          [{'name': 'OntoNotes 5', 'url': 'https://catalog.ldc.upenn.edu/LDC2013T19', 'license': 'commercial (licensed by Explosion)'}, {'name': 'Common Crawl'}]\n",
            "pipeline         ['tagger', 'parser', 'ner']   \n",
            "version          2.2.5                         \n",
            "spacy_version    >=2.2.2                       \n",
            "parent_package   spacy                         \n",
            "labels           {'tagger': ['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``'], 'parser': ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp'], 'ner': ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']}\n",
            "vectors          {'width': 300, 'vectors': 684831, 'keys': 684830, 'name': 'en_core_web_lg.vectors'}\n",
            "source           /usr/local/lib/python3.7/dist-packages/en_core_web_lg\n",
            "\n",
            "Loading model en_core_web_lg\n",
            "🌋 Parsing utterances and filling docs with use_gold_mentions=False\n",
            "2504it [00:14, 178.69it/s]\n",
            "=> read_corpus time elapsed 26.04082179069519\n",
            "🌋 Extracting mentions features with 1 job(s)\n",
            "100% 64/64 [00:02<00:00, 27.49it/s]\n",
            "🌋 Building and gathering array with 1 job(s)\n",
            "100% 64/64 [00:07<00:00,  8.19it/s]\n",
            "100% 74/74 [00:00<00:00, 1988.52it/s]\n",
            "Building numpy array for mentions_features length 6090\n",
            "Saving numpy mentions_features size (6090, 6)\n",
            "Building numpy array for mentions_labels length 6090\n",
            "Saving numpy mentions_labels size (6090, 1)\n",
            "Building numpy array for mentions_pairs_length length 6090\n",
            "Saving numpy mentions_pairs_length size (6090, 1)\n",
            "Building numpy array for mentions_pairs_start_index length 6090\n",
            "Saving numpy mentions_pairs_start_index size (6090, 1)\n",
            "Building numpy array for mentions_spans length 6090\n",
            "Saving numpy mentions_spans size (6090, 250)\n",
            "Building numpy array for mentions_words length 6090\n",
            "Saving numpy mentions_words size (6090, 8)\n",
            "Building numpy array for pairs_ant_index length 315432\n",
            "Saving numpy pairs_ant_index size (315432, 1)\n",
            "Building numpy array for pairs_features length 315432\n",
            "Saving numpy pairs_features size (315432, 9)\n",
            "Building numpy array for pairs_labels length 315432\n",
            "Saving numpy pairs_labels size (315432, 1)\n",
            "Saving pickle locations size 74\n",
            "Saving pickle conll_tokens size 74\n",
            "Saving pickle spacy_lookup size 74\n",
            "Saving pickle doc size 74\n",
            "=> build_and_gather_multiple_arrays time elapsed 12.302427768707275\n",
            "🌋 Building tunable vocabulary matrix from static vocabulary\n",
            "🌋 Saving vocabulary\n",
            "🌋 Saving vocabulary\n",
            "Saving tunable voc, size: (34288, 50)\n",
            "Saving static voc, size: (103144, 50)\n",
            "=> save_vocabulary time elapsed 0.33161449432373047\n",
            "=> total time elapsed 38.675034284591675\n",
            "🌋 Building key file from corpus\n",
            "Saving in /content/dev/key.txt\n",
            "In /content/dev\n",
            "In /content/dev/.ipynb_checkpoints\n",
            "In /content/dev/numpy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBqFGC2FotQZ",
        "outputId": "06d46929-8997-483e-c77a-e24b3c590e0d"
      },
      "source": [
        "!python -m neuralcoref.train.learn --train /content/train --eval /content/dev"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "Namespace(all_pairs_epoch=200, all_pairs_l2=1e-06, all_pairs_lr=0.0002, batchsize=20000, checkpoint_file=None, conll_eval_interval=10, conll_train_interval=20, costfl=0.4, costfn=0.8, costs={'FN': 0.8, 'FL': 0.4, 'WL': 1.0}, costwl=1.0, cuda=False, eval='/content/dev/numpy/', evalkey='/content/dev/key.txt', h1=1000, h2=500, h3=500, lazy=True, log_interval=10, min_lr=2e-08, numworkers=8, on_eval_decrease='nothing', patience=3, ranking_epoch=200, ranking_l2=1e-05, ranking_lr=2e-06, save_path='/content/drive/MyDrive/neuralcoref-master/neuralcoref/train/checkpoints/May20_20-16-16_8e141abbce67_', seed=1111, startstage=None, startstep=None, top_pairs_epoch=200, top_pairs_l2=1e-05, top_pairs_lr=0.0002, train='/content/train/numpy/', trainkey='/content/train/key.txt', weights=None)\n",
            "Training for 200 200 200 epochs\n",
            "loading /content/train/numpy/tuned_word_embeddings.npy\n",
            "torch.Size([34288, 50])\n",
            "loading /content/train/numpy/tuned_word_vocabulary.txt\n",
            "🏝 Loading Dataset at /content/train/numpy/\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/drive/MyDrive/neuralcoref-master/neuralcoref/train/learn.py\", line 565, in <module>\n",
            "    run_model(args)\n",
            "  File \"/content/drive/MyDrive/neuralcoref-master/neuralcoref/train/learn.py\", line 136, in run_model\n",
            "    dataset = NCDataset(args.train, args)\n",
            "  File \"/content/drive/MyDrive/neuralcoref-master/neuralcoref/train/dataset.py\", line 82, in __init__\n",
            "    data_path + file_name, mmap_mode=\"r\" if params.lazy else None\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\", line 437, in load\n",
            "    return format.open_memmap(file, mode=mmap_mode)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/numpy/lib/format.py\", line 858, in open_memmap\n",
            "    raise ValueError(msg)\n",
            "ValueError: Array can't be memory-mapped: Python objects in dtype.\n",
            "Reading mentions_pairs_length.npy, mentions_labels.npy, mentions_pairs_start_index.npy, pairs_features.npy, tuned_word_embeddings.npy, mentions_words.npy, mentions_spans.npy, mentions_features.npy, "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}